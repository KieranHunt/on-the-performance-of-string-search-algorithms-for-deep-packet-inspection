\documentclass[11pt]{article}
\usepackage[round]{natbib}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{subfig}
\usepackage{pgf}
\usepackage[margin=1in]{geometry}

\begin{document}

\title{On the Performance of String Search Algorithms for Deep Packet Inspection}
\author{Kieran Hunt}
\date{\today}
\maketitle

\section{Introduction}

In Deep Packet Inspection today, systems are generally built on top of expensive custom hardware. Making changes to such a system (such as horizontally scaling) is often arduous, time consuming and expensive. Deep Packet Inspection via software means is usually slower but does provide some benefit: adding or removing capacity to perform inspection is often as simple as adding or removing hosts doing the inspection. String search algorithms have long been of interest to the field of computer science and as a result a substantial number of search algorithms exist. This paper looks to ask which of these string search algorithms perform best at deep packet inspection and how does their performance compare to that of their intended design.

Relevant Deep Packet Inspection terminology is as follows:
\begin{itemize}
  \item \textbf{Packet}: Data representing a TCP/IP stack packet. This includes information from the network to the application layer.
  \item \textbf{Packet Capture File (PCAP)}: A file containing packets. These packets were generally captured by recording a network interface.
  \item \textbf{Deep Packet Inspection (DPI)}: the process by which a packet, or stream of packets, is analysed for the presence of predefined patterns.
\end{itemize}

In order to properly test these string search algorithms, a system was designed to accurately compare each algorithm for inspection of both packet captures as well as textual inputs such as text files.

In the system for comparing string search algorithms, the following terminology is relevant:
\begin{itemize}
  \item \textbf{Input}: This is the interface through which the system reads in either the packets or textual Input. Algorithms can request a single byte from the input, the length of the Input or the entire Input itself.
  \item \textbf{Rule}: The system searches through the input for a given Rule. Rules are very similar to input in that a single byte of information, the length of the rule or the entire rule itself can be requested. 
  \item \textbf{Algorithm}: This represents a string search algorithm and is the means by which the system interacts with all of the algorithms. It has a single interface for performing a search where the Input is supplied and a Result is returned.
  \item \textbf{Result}: This represents the result of the inspection of a single Input. In it is the start and end times of the inspection, the Rules, Input and the location (if any) in the Input where the Rules were matched.
\end{itemize}

Each of the string search algorithms have known performance (algorithm complexity known as big O) usually related to the length of the string being searched. The results of the following experiments should follow the predicted complexity of string search algorithms. Algorithm complexity often only provides insight into large variations in Input length (differing orders of magnitude) whereas in packet data has a limited range of Input lengths and so it may come down to minutiae within the algorithms themselves rather than their overall complexity.

\section{String Search Algorithms}

A vast collection of string search algorithms has been amassed by \citet{charras2004} and from that a selection of algorithms was chosen to implement, benchmark and then compare.
Table \ref{table-algorithms} has a list of each algorithm, the year it was publish, its author(s) and the time complexity of searching with that algorithm.

\begin{table}[!hbt]
\resizebox{\textwidth}{!}{
\begin{tabular}{l|lll}
  Algorithm & Year & Author(s) & Time Complexity \\
  \hline
  Naive &  ? & ? & O(mn) \\
  MorrisPratt & \citeyear{morris1970} & \citeauthor{morris1970} & O(n + m) \\
  KnuthMorrisPratt & \citeyear{knuth1977} & \citeauthor{knuth1977} & O(n + m) \\
  BoyerMoore & \citeyear{boyer1977} & \citeauthor{boyer1977} & O(nm) \\
  Horspool & \citeyear{horspool1980} & \citeauthor{horspool1980} & O(n + m) \\
  ApostolicoGiancarlo & \citeyear{apostolico1986} & \citeauthor{apostolico1986} & O(n) \\
  RabinKarp & \citeyear{karp1987} & \citeauthor{karp1987} & O(mn) \\
  ZhuTakaoka & \citeyear{feng1987} & \citeauthor{feng1987} & O(mn) \\
  QuickSearch & \citeyear{sunday1990} & \citeauthor{sunday1990} & O(mn) \\
  Smith & \citeyear{smith1991} & \citeauthor{smith1991} & O(mn) \\
  ApostolicoCrochemore & \citeyear{apostolico1991} & \citeauthor{apostolico1991} & O(n) \\
  Colussi & \citeyear{colussi1991} & \citeauthor{colussi1991} & O(n) \\
  Raita & \citeyear{raita1991} & \citeauthor{raita1991} & O(nm) \\
  GalilGiancarlo & \citeyear{galil1991} & \citeauthor{galil1991} & O(n) \\
  Bitap (Shift Or) & \citeyear{baezayates1992} & \citeauthor{baezayates1992} & O(n) \\
  NotSoNaive & \citeyear{hancart1993} & \citeauthor{hancart1993} & O(nm) \\
  Simon & \citeyear{simon1994} & \citeauthor{simon1994} & O(n + m) \\
  TurboBoyerMoore & \citeyear{crochemore1994} & \citeauthor{crochemore1994} & O(n) \\
  ReverseColussi & \citeyear{colussi1994} & \citeauthor{colussi1994} & O(n)
\end{tabular}}
\caption{Implemented string search algorithms for the purpose of comparison against a packet dataset. Under time complexity, \textit{n} represents the length of the Input and \textit{m} represents the length of a Rule. The time complexity is multiplied by a factor equal to the number of Rules.}
\label{table-algorithms}
\end{table}

The time complexities in Table \ref{table-algorithms} should only serve as a basic indication of the speed of an algorithm. Big-O notations generally strip off any factors and so two algorithms may appear to have the same time complexities but in practice their speeds differ greatly because of this unknown factor.

Another point to note is that because these algorithms were designed to search for a single Rule, their Big-O notations only reflect that property. For multiple rules, and a serial-style approach, these algorithms need to run back-to-back; this increases increases their time to process by a factor equal to the number of Rules. For a parallel-style approach searches for different rules using the same algorithm can be run at the same time. The speed at which a search for a complete set of Rules completes is then both related to the algorithmic complexity of the search as well as the number of rules which are being searched for at the same time.

An upper bound for the number of concurrent searches for different Rules with the same Algorithm and Input exists. That upper bound is defined by the number of processor cores that are available for use.

\section{Method}

As mentioned previously, a system was developed to allow accurate running and performance measurement of each of the implemented algorithms (in Table \ref{table-algorithms}). The system takes a json file as input (See Listing \ref{listing-testConfiguration.json}). For the test, the following configuration was chosen:
\begin{itemize}
  \item \texttt{algorithms} - a list of all implemented Algorithms (See Table \ref{table-algorithms}) 
  \item \texttt{rules} - string-based rules covering the twenty most popular websites \citep{alexa2016} - just their domain names were taken - as well as the twenty most popular words in the English language \citep{oed2016}.
  \item \texttt{inputs} - a dataset of DNS traffic was selected and a subset of 10000 packets was extracted. This was labeled \texttt{smallcapture.pcap}. A second Input was also selected, this is the complete \textit{Alice in Wonderland} by Lewis Carol. This Input was labeled \texttt{alice.txt}. The system treats text and pcap input files differently. Text input files are used to create a single Input object representing all information in that file. Pcap input files are split up into individual packet objects and each packet represents a single Input.
  \item \texttt{times} - the tests were each run 20 times.
  \item \texttt{threadCount} - drastic speed increases are made possible by splitting the work of the Algorithms across multiple threads. The machine used to perform the test has 24 useable cores and so a max \texttt{threadCount} of 18 was chosen to best make use of those cores.
\end{itemize}


\begin{lstlisting}[caption = {Sample testConfiguration.json}, label = {listing-testConfiguration.json}]
{
  "algorithms": [
    "Naive",
    "MorrisPratt",
    ...
    "ReverseColussi"
  ],
  "rules": [
    "time",
    "person",
    ...
    "msn"
  ],
  "inputs": [
    {
      "type": "pcap",
      "location": "smallcapture.pcap"
    },
    {
      "type": "text",
      "location": "alice.txt"
    }
  ],
  "times": 20,
  "threadCount": 18
}
\end{lstlisting}

\section{Results}

The test generated 3400340 result objects. Each result object looked very similar to Listing \ref{listing-testresult}. Containing information about the start and end times (in nanoseconds), the elapsed time (the end time - the start time), the rules searched for, the locations that rules were matched at, the algorithm used, the input file, the Input ID, the run number and the ID of that run.

\begin{lstlisting}[caption = {Sample test Result}, label = {listing-testresult}]
[
    {
        "start": 206079193307938,
        "end": 206079207132342,
        "elapsed": 13824404,
        "rules": [
            "time",
            "person",
            ...
            "msn"
        ],
        "locations": [1, 2, ..., n],
        "algorithm": "Smith",
        "inputFile": "smallcapture.pcap",
        "inputID": "bf75faa5",
        "runNumber": 1,
        "runId": "d9b4a28aefbd"
    },
    ...
]
\end{lstlisting}

\section{Analysis}

Figure \ref{figure-barmeanprocessingsmallcapture} shows a comparison of processing times for each algorithm over each of the packets in PCAP dataset. Figure \ref{figure-barmeanprocessingalice} gives the same comparison albeit for the \textit{Alice in Wonderland} input file.

\begin{figure}[t]
    \centering
    \subfloat[Mean Input processing times of each Algorithm for \textit{PCAP Dataset}.]{\includegraphics[width=0.85\textwidth]{images/bar_graph_mean_one_input_smallcapture_pcap}\label{figure-barmeanprocessingsmallcapture}}
    
    \subfloat[Mean Input processing times of each Algorithm for \textit{Alice in Wonderland}.]{\includegraphics[width=0.85\textwidth]{images/bar_graph_mean_one_input_alice_txt}\label{figure-barmeanprocessingalice}}
    \caption{Comparison of the mean processing times for both the packet and textual datasets.}
\end{figure}

Taking into account the processing speed of each algorithm from both Figure \ref{figure-barmeanprocessingsmallcapture} and \ref{figure-barmeanprocessingalice} we are able to list the algorithms in order of their processing speed - fastest first: Horspool, Quicksearch, Raita, ReverseColussi, BoyerMoore, ZhuTakaoka, Smith, Colussi, KnuthMorrisPratt, TurboBoyerMoore, ApostolicoGiancarlo, MorrisPratt, Naive, Simon, Bitap, NotSoNaive, RabinKarp. 

Although the order of speediness changes between Figure \ref{figure-barmeanprocessingsmallcapture} and \ref{figure-barmeanprocessingalice}, the position of the algorithms varies by less than four places.

The relative speed difference of each algorithm in Figure \ref{figure-barmeanprocessingsmallcapture} appears to be quite small. By increasing the length of the input data (in this case by using the \textit{Alice in Wonderland} text. Figure \ref{figure-barmeanprocessingalice}) we are able to accentuate the differences.

\subsection{Real World Packet Processing Speeds}

For the packet data, most algorithms have a mean packet processing speed of anywhere between 0.1 and 0.2 milliseconds.

The average processing speed of 0.15ms as well as an average packet length of xxx means that a rough line speed of yyy is achievable. This speed is obviously much less than what would be required of a modern Deep Packet Inspection system. The goal of this research is to develop an understand of the algorithms' relative speed.

\section{Conclusion}

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
